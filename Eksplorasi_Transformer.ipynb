{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rizkyabdillah127/transformer_translation/blob/main/Eksplorasi_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tugas Eksplorasi Transformer\n",
        "\n",
        "122140127\n",
        "\n",
        "122140165\n",
        "\n",
        "122140145\n",
        "\n",
        "\n",
        "Tahapan Awal Yang Kami Lakukan Adalah\n",
        "\n",
        "Menginstal atau memperbarui spaCy, yaitu library NLP (Natural Language Processing) yang digunakan untuk tokenisasi, lemmatization, dan pemrosesan teks lainnya.\n",
        "\n",
        "Mengunduh model bahasa Inggris kecil (small) untuk spaCy.\n",
        "Model ini berisi kamus, aturan grammar, dan vektor kata untuk bahasa Inggris.\n",
        "\n",
        "Mengunduh model bahasa Prancis kecil (small) untuk spaCy.\n",
        "Digunakan agar spaCy bisa memproses teks berbahasa Prancis dengan benar (misalnya untuk tokenisasi).\n",
        "\n",
        "Menginstal PyTorch versi 2.3.0 dan TorchText versi 0.18.0,\n",
        "dua library utama untuk membuat dan melatih model deep learning (Transformer) dan memproses teks secara efisien."
      ],
      "metadata": {
        "id": "x2sNSvKpk8Eu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBF0QA0S9G0w",
        "outputId": "2c493ba6-299c-410c-aa5b-9b423ded427e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Requirement already satisfied: torchtext==0.18.0 in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm\n",
        "!pip install torch==2.3.0 torchtext==0.18.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagian kode ini adalah tahap **impor pustaka (library)**, yang berfungsi seperti \"mempersiapkan kotak peralatan\" sebelum Anda memulai proyek NLP. Pertama, Anda mengimpor **PyTorch** (`torch`) sebagai mesin *deep learning* utama, beserta modul-modul intinya: `torch.nn` (untuk membangun arsitektur model seperti *layers* dan *loss functions*), `torch.optim` (untuk algoritma pelatihan seperti Adam/SGD), serta `DataLoader` dan `Dataset` (untuk mengelola dan memuat data Anda secara efisien). Khusus untuk NLP, Anda mengimpor `pad_sequence` untuk menyeragamkan panjang kalimat yang berbeda-beda. Selanjutnya, Anda mengimpor alat bantu pra-pemrosesan data: **Pandas** (`pd`) untuk memuat dan memanipulasi data (misalnya dari file CSV), **SpaCy** untuk melakukan tokenisasi (memecah teks menjadi kata), dan **TorchText** untuk membuat kosakata (mengubah kata menjadi angka). Terakhir, Anda mengimpor utilitas penting: `train_test_split` dari **Scikit-learn** (sklearn) untuk membagi data latih dan validasi (penting untuk mengecek performa model), serta pustaka standar Python (`math`, `time`, `os`, `random`) untuk operasi matematika, pengaturan waktu, manajemen file, dan memastikan hasil yang konsisten (reproducibility)."
      ],
      "metadata": {
        "id": "Y0CjKXD9k6bE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from sklearn.model_selection import train_test_split # Penting untuk ValLoss/ValAcc\n",
        "\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import random"
      ],
      "metadata": {
        "id": "SaoeHUjI-Hod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya ada perintah khusus untuk lingkungan **Google Colab**, yang berfungsi untuk **menghubungkan (me-\"mount\") Google Drive Anda** ke dalam *runtime* (mesin virtual) Colab.\n",
        "\n",
        "Secara sederhana:\n",
        "\n",
        "1.  **`from google.colab import drive`**: Mengimpor pustaka `drive` yang disediakan oleh Google Colab.\n",
        "2.  **`drive.mount('/content/drive')`**: Memerintahkan Colab untuk \"memasang\" Google Drive Anda.\n",
        "\n",
        "Setelah Anda menjalankan kode ini, Colab akan meminta izin Anda (biasanya lewat *pop-up*) untuk mengakses file di Google Drive Anda. Setelah Anda memberi izin, semua file dan folder di Google Drive Anda akan muncul dan bisa diakses dari dalam Colab melalui direktori `/content/drive`.\n",
        "\n",
        "Ini sangat penting untuk:\n",
        "* **Membaca dataset** yang telah Anda simpan di Drive.\n",
        "* **Menyimpan model** yang sudah dilatih agar tidak hilang saat *runtime* Colab mati."
      ],
      "metadata": {
        "id": "qIp-UyBrnFqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-nSAqHCTAFqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e052637-59ce-4456-fb9c-52e166f9f953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagian kode dibawah ini melakukan dua tugas penyiapan (setup) yang krusial sebelum memulai pelatihan model *deep learning*:\n",
        "\n",
        "**1. memilih perangkat keras (hardware) terbaik**\n",
        "\n",
        " yang tersedia secara otomatis. Perintah `DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')` memeriksa apakah ada GPU NVIDIA (`'cuda'`) yang bisa digunakan. Jika ya, `DEVICE` akan diatur ke `'cuda'`, karena pelatihan *deep learning* jauh lebih cepat di GPU. Jika tidak ada GPU, `DEVICE` akan diatur ke `'cpu'`. Pesan konfirmasi kemudian dicetak untuk memberitahu Anda perangkat mana yang aktif.\n",
        "\n",
        " **2. mengatur \"seed\" untuk reproduktifitas (reproducibility)**.\n",
        "\n",
        " Dalam *deep learning*, banyak operasi (seperti inisialisasi bobot model atau pengacakan data) bersifat acak. Dengan menetapkan `SEED` (misalnya `1234`) dan memberikannya ke `random.seed`, `torch.manual_seed`, dan `torch.cuda.manual_seed`, Anda \"mengunci\" keacakan ini. Ditambah dengan `torch.backends.cudnn.deterministic = True`, ini memastikan bahwa setiap kali Anda menjalankan kode ini dari awal, Anda akan mendapatkan hasil yang *persis* sama, yang sangat penting untuk eksperimen yang valid dan perbandingan antar model."
      ],
      "metadata": {
        "id": "7lTyG1kmn33s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Menggunakan device: {DEVICE}\")\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "L05qElaMAJxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881750ba-4d22-4b2f-84b8-14c674156847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Menggunakan device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ini adalah bagian **Persiapan Data (Data Preparation)**, salah satu tahap paling krusial dalam proyek *machine translation*.\n",
        "\n",
        "Tujuan utama dari blok kode ini adalah untuk mengambil file teks mentah (Bahasa Inggris dan Prancis) dan mengubahnya menjadi format terstruktur yang siap \"dimakan\" oleh model *deep learning* (PyTorch).\n",
        "\n",
        "Berikut adalah rincian langkah-langkahnya:\n",
        "\n",
        "## 1. Memuat Tokenizer\n",
        "* **Apa:** Kode ini memuat dua model bahasa Spacy yang telah dilatih sebelumnya (`en_core_web_sm` untuk Inggris dan `fr_core_news_sm` untuk Prancis).\n",
        "* **Kenapa:** Kita memerlukan \"tokenizer\" untuk memecah kalimat mentah (string) menjadi daftar kata-kata individual (token). Contoh: `\"hello world\"` menjadi `['hello', 'world']`. Dua fungsi, `tokenize_en` dan `tokenize_fr`, dibuat untuk melakukan tugas ini.\n",
        "\n",
        "\n",
        "## 2. Memuat dan Memvalidasi Dataset\n",
        "* **Apa:** Kode ini mencoba membaca dua file teks (`.csv` yang diperlakukan sebagai teks biasa) dari Google Drive Anda. Satu file berisi kalimat-kalimat Bahasa Inggris dan satu lagi berisi terjemahannya dalam Bahasa Prancis.\n",
        "* **Kenapa:** Ini adalah data mentah kita. Kode ini mengasumsikan bahwa data tersebut adalah *paralel*, artinya baris ke-5 di file Inggris adalah terjemahan dari baris ke-5 di file Prancis.\n",
        "* **Penanganan Error:**\n",
        "    * Jika file **tidak ditemukan**, kode ini secara cerdas membuat *dummy dataset* (data palsu) kecil agar sisa kode tetap bisa berjalan tanpa error.\n",
        "    * Jika jumlah baris **tidak sama**, kode ini akan memotong dataset ke jumlah baris minimum agar tetap sinkron.\n",
        "* **Output:** Sebuah **Pandas DataFrame** `df` dibuat, di mana setiap baris berisi satu pasang kalimat (`'en'` dan `'fr'`).\n",
        "\n",
        "\n",
        "## 3. Pemisahan Data (Train & Validation)\n",
        "* **Apa:** Dataset `df` dipecah menjadi dua bagian menggunakan `train_test_split`.\n",
        "* **Kenapa:** Ini adalah praktik standar *machine learning*.\n",
        "    * **Training Data (`train_df`):** Bagian terbesar (di sini 90%) yang digunakan untuk *mengajari* model.\n",
        "    * **Validation Data (`val_df`):** Bagian kecil (10%) yang \"disembunyikan\" dari model selama pelatihan. Data ini dipakai untuk *menguji* seberapa baik model bekerja pada data yang belum pernah dilihatnya (untuk mengecek *Val Loss/Accuracy*).\n",
        "* `random_state=SEED` digunakan agar hasil pemisahannya selalu sama setiap kali kode dijalankan.\n",
        "\n",
        "\n",
        "## 4. Membangun Kosakata (Vocabulary)\n",
        "* **Apa:** Kode ini membangun dua \"kamus\" (`vocab_src` untuk Inggris dan `vocab_tgt` untuk Prancis) dari data pelatihan.\n",
        "* **Kenapa:** Model *deep learning* tidak mengerti kata \"hello\"; mereka hanya mengerti **angka**. *Vocabulary* ini memetakan setiap kata unik ke sebuah angka (indeks).\n",
        "* **Detail Penting:**\n",
        "    * **`min_freq=2`**: Kata yang hanya muncul sekali di data latih akan dibuang (dianggap sebagai `<unk>`). Ini membantu mengurangi ukuran *vocabulary* dan membuang kata-kata yang mungkin *typo*.\n",
        "    * **`special_symbols`**: Empat token khusus ditambahkan:\n",
        "        * `<unk>`: Untuk kata yang tidak dikenal (tidak ada di *vocabulary*).\n",
        "        * `<pad>`: (Padding) Untuk \"mengisi\" kalimat yang lebih pendek agar sama panjangnya dalam satu *batch*.\n",
        "        * `<bos>`: (Begin of Sentence) Tanda penanda awal kalimat.\n",
        "        * `<eos>`: (End of Sentence) Tanda penanda akhir kalimat.\n",
        "\n",
        "\n",
        "## 5. Membuat `Dataset` Kustom\n",
        "* **Apa:** Sebuah `class` Python bernama `TranslationDataset` dibuat menggunakan `Dataset` milik PyTorch.\n",
        "* **Kenapa:** Ini adalah \"cetak biru\" (blueprint) standar PyTorch untuk data Anda. Ini memberi tahu PyTorch cara mengambil **satu** item data.\n",
        "* **Fungsi `__getitem__`**: Ini adalah intinya. Ketika diminta data di indeks `idx`, fungsi ini akan:\n",
        "    1.  Mengambil kalimat 'en' dan 'fr'.\n",
        "    2.  Melakukan tokenisasi (memecahnya jadi kata).\n",
        "    3.  Mengubah kata-kata menjadi angka menggunakan *vocabulary* yang tadi dibuat.\n",
        "    4.  Menambahkan token `<bos>` di awal dan `<eos>` di akhir.\n",
        "    5.  Mengembalikan hasilnya sebagai `torch.tensor`.\n",
        "\n",
        "\n",
        "## 6. Membuat `DataLoader`\n",
        "* **Apa:** Kode ini membungkus `TranslationDataset` (untuk data latih dan validasi) dengan `DataLoader`.\n",
        "* **Kenapa:** `DataLoader` adalah \"mesin\" yang secara otomatis mengambil data dari `Dataset` dan mengaturnya untuk pelatihan. Ia menangani:\n",
        "    * **Batching**: Mengelompokkan beberapa kalimat (misalnya 64) menjadi satu *batch* untuk efisiensi GPU.\n",
        "    * **Shuffling**: Mengacak urutan data latih di setiap *epoch* (penting agar model tidak \"menghafal\" urutan data).\n",
        "    * **Collation (`collate_fn`)**: Ini sangat penting. Karena kalimat dalam satu *batch* punya panjang berbeda, fungsi `collate_fn` **menambahkan *padding* (`<pad>`)** ke kalimat yang lebih pendek sehingga semua kalimat dalam *batch* memiliki panjang yang sama. Ini mengubahnya menjadi *tensor* persegi yang rapi.\n",
        "* **`batch_first=False`**: Ini adalah pengaturan penting yang berarti *tensor* yang dihasilkan akan memiliki bentuk `[panjang_kalimat, batch_size]`, format yang umum digunakan oleh model Transformer.\n",
        "\n",
        "Singkatnya, **BAGIAN 1** ini mengubah dua file `.csv` mentah menjadi dua objek `DataLoader` (`train_dataloader` dan `val_dataloader`) yang sangat efisien dan siap dimasukkan ke dalam model PyTorch untuk pelatihan."
      ],
      "metadata": {
        "id": "qvVVufAgoY0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# BAGIAN 1: FOKUS PENILAIAN 1 - DATA PREPARATION (BOBOT 20%)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Memulai Fokus Penilaian 1: Data Preparation ---\")\n",
        "\n",
        "# --- 1.1. Memuat Tokenizer Spacy ---\n",
        "try:\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "    spacy_fr = spacy.load('fr_core_news_sm') # <-- BERUBAH: Muat model Prancis\n",
        "    print(\"Tokenizer Spacy (en/fr) berhasil dimuat.\")\n",
        "except IOError:\n",
        "    print(\"Error: Model Spacy tidak ditemukan. Pastikan instalasi di atas berhasil.\")\n",
        "    exit()\n",
        "\n",
        "# Fungsi tokenizer (Source: English)\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(str(text))]\n",
        "\n",
        "# Fungsi tokenizer (Target: French)\n",
        "def tokenize_fr(text): # <-- BERUBAH: Nama fungsi dan model\n",
        "    return [tok.text for tok in spacy_fr.tokenizer(str(text))]\n",
        "\n",
        "# --- 1.2. Memuat Dataset dari Google Drive (Format File Teks Paralel) ---\n",
        "# PENTING: Pastikan kedua file ada di folder 'MyDrive' Anda.\n",
        "EN_FILE_PATH = '/content/drive/MyDrive/DictionaryDataset/small_vocab_en.csv'\n",
        "FR_FILE_PATH = '/content/drive/MyDrive/DictionaryDataset/small_vocab_fr.csv'\n",
        "\n",
        "if not os.path.exists(EN_FILE_PATH) or not os.path.exists(FR_FILE_PATH):\n",
        "    print(f\"Error: Salah satu file tidak ditemukan.\")\n",
        "    print(f\"Pastikan '{EN_FILE_PATH}' dan '{FR_FILE_PATH}' ada.\")\n",
        "    # Membuat data dummy agar kode tetap berjalan untuk demo\n",
        "    df = pd.DataFrame({\n",
        "        'en': ['hello world', 'how are you', 'good morning', 'i am a student'],\n",
        "        'fr': ['bonjour le monde', 'comment vas-tu', 'bonjour', 'je suis étudiant']\n",
        "    })\n",
        "else:\n",
        "    print(f\"Memuat dataset dari: {EN_FILE_PATH} dan {FR_FILE_PATH}\")\n",
        "\n",
        "    # Baca file Inggris\n",
        "    with open(EN_FILE_PATH, 'r', encoding='utf-8') as f:\n",
        "        en_sentences = [line.strip() for line in f]\n",
        "\n",
        "    # Baca file Prancis\n",
        "    with open(FR_FILE_PATH, 'r', encoding='utf-8') as f:\n",
        "        fr_sentences = [line.strip() for line in f]\n",
        "\n",
        "    # Pastikan kedua file memiliki jumlah baris yang sama\n",
        "    if len(en_sentences) != len(fr_sentences):\n",
        "        print(f\"Error: Jumlah baris tidak sama! EN: {len(en_sentences)}, FR: {len(fr_sentences)}\")\n",
        "        # (Lanjutkan dengan data yang dipotong agar demo tetap jalan)\n",
        "        min_len = min(len(en_sentences), len(fr_sentences))\n",
        "        en_sentences = en_sentences[:min_len]\n",
        "        fr_sentences = fr_sentences[:min_len]\n",
        "\n",
        "    # Buat DataFrame\n",
        "    df = pd.DataFrame({'en': en_sentences, 'fr': fr_sentences})\n",
        "\n",
        "print(f\"Dataset berhasil dimuat. Jumlah total pasangan kalimat: {len(df)}\")\n",
        "\n",
        "# --- 1.3. Pemisahan Data (Train & Validation) ---\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=SEED)\n",
        "print(f\"Data dibagi: {len(train_df)} train, {len(val_df)} validasi.\")\n",
        "\n",
        "# --- 1.4. Pembangunan Vocabulary ---\n",
        "# Dok: Karena ini 'small_vocab', kita bisa gunakan min_freq=2\n",
        "def yield_tokens(data_iter, tokenizer):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Token spesial\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "# Vocabulary untuk Bahasa Inggris (Source)\n",
        "vocab_src = build_vocab_from_iterator(\n",
        "    yield_tokens(train_df['en'], tokenize_en),\n",
        "    min_freq=2, # Kita set 2 untuk membuang kata yang sangat langka\n",
        "    specials=special_symbols,\n",
        "    special_first=True\n",
        ")\n",
        "vocab_src.set_default_index(UNK_IDX)\n",
        "\n",
        "# Vocabulary untuk Bahasa Prancis (Target)\n",
        "vocab_tgt = build_vocab_from_iterator(\n",
        "    yield_tokens(train_df['fr'], tokenize_fr), # <-- BERUBAH: 'fr' dan tokenize_fr\n",
        "    min_freq=2,\n",
        "    specials=special_symbols,\n",
        "    special_first=True\n",
        ")\n",
        "vocab_tgt.set_default_index(UNK_IDX)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_src)\n",
        "TGT_VOCAB_SIZE = len(vocab_tgt)\n",
        "print(f\"Ukuran Vocab Source (EN): {SRC_VOCAB_SIZE}\")\n",
        "print(f\"Ukuran Vocab Target (FR): {TGT_VOCAB_SIZE}\") # <-- BERUBAH: FR\n",
        "\n",
        "# --- 1.5. Class Kustom PyTorch Dataset ---\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, df, vocab_src, vocab_tgt, tokenize_src, tokenize_tgt):\n",
        "        self.df = df\n",
        "        self.vocab_src = vocab_src\n",
        "        self.vocab_tgt = vocab_tgt\n",
        "        self.tokenize_src = tokenize_src\n",
        "        self.tokenize_tgt = tokenize_tgt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        src_text = row['en']\n",
        "        tgt_text = row['fr'] # <-- BERUBAH: 'fr'\n",
        "\n",
        "        src_tensor = torch.tensor([BOS_IDX] + [self.vocab_src[token] for token in self.tokenize_src(src_text)] + [EOS_IDX], dtype=torch.long)\n",
        "        tgt_tensor = torch.tensor([BOS_IDX] + [self.vocab_tgt[token] for token in self.tokenize_tgt(tgt_text)] + [EOS_IDX], dtype=torch.long)\n",
        "        return src_tensor, tgt_tensor\n",
        "\n",
        "# --- 1.6. Collate Function dan DataLoader ---\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(src_sample)\n",
        "        tgt_batch.append(tgt_sample)\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# MENGURANGI BATCH SIZE untuk membantu mencegah OOM Error\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buat instance Dataset\n",
        "train_dataset = TranslationDataset(train_df, vocab_src, vocab_tgt, tokenize_en, tokenize_fr) # <-- BERUBAH\n",
        "val_dataset = TranslationDataset(val_df, vocab_src, vocab_tgt, tokenize_en, tokenize_fr) # <-- BERUBAH\n",
        "\n",
        "# Buat DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "print(f\"DataLoader berhasil dibuat dengan Batch Size: {BATCH_SIZE}\")\n",
        "print(\"--- Selesai Fokus Penilaian 1 ---\")"
      ],
      "metadata": {
        "id": "cpCInONmAR0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56908dc3-b9cc-49b0-bf01-db4c45c19482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Memulai Fokus Penilaian 1: Data Preparation ---\n",
            "Tokenizer Spacy (en/fr) berhasil dimuat.\n",
            "Memuat dataset dari: /content/drive/MyDrive/DictionaryDataset/small_vocab_en.csv dan /content/drive/MyDrive/DictionaryDataset/small_vocab_fr.csv\n",
            "Dataset berhasil dimuat. Jumlah total pasangan kalimat: 137860\n",
            "Data dibagi: 124074 train, 13786 validasi.\n",
            "Ukuran Vocab Source (EN): 205\n",
            "Ukuran Vocab Target (FR): 338\n",
            "DataLoader berhasil dibuat dengan Batch Size: 64\n",
            "--- Selesai Fokus Penilaian 1 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BAGIAN 2**, inti dari proyek ini, di mana Anda **membangun arsitektur model Transformer** itu sendiri. Anda tidak melatihnya di sini, tetapi Anda mendefinisikan \"cetak biru\" atau kerangka dari model yang akan melakukan penerjemahan.\n",
        "\n",
        "Berikut adalah penjelasan dari setiap komponen utamanya:\n",
        "\n",
        "## 1. Komponen Pendukung: `TokenEmbedding` dan `PositionalEncoding`\n",
        "\n",
        "Sebelum membangun model utamanya, Anda perlu dua komponen penting:\n",
        "\n",
        "* **`TokenEmbedding` (Kamus Makna)**: Ini adalah *layer* sederhana yang mengubah input Anda (yang masih berupa angka/indeks, misal: `[2, 56, 34]`) menjadi **vektor makna (embeddings)**. `nn.Embedding` bertindak seperti tabel pencarian (lookup table) raksasa di mana setiap angka indeks kata (misal, 56 untuk kata \"hello\") dipetakan ke vektor padat (misal, berukuran 512) yang merepresentasikan \"makna\" dari kata tersebut.\n",
        "* **`PositionalEncoding` (GPS Urutan Kata)** : Ini adalah \"trik\" jenius dari arsitektur Transformer. Tidak seperti RNN, Transformer memproses semua kata dalam kalimat secara bersamaan (paralel), sehingga ia **tidak tahu urutan kata** (misal, bedanya \"kucing makan tikus\" vs \"tikus makan kucing\"). *Positional Encoding* membuat \"vektor posisi\" yang unik untuk setiap urutan (posisi 1, 2, 3, dst.) menggunakan fungsi matematika `sin` dan `cos`. Vektor posisi ini kemudian **ditambahkan** ke vektor makna (`TokenEmbedding`) untuk memberikan model informasi kontekstual tentang di mana setiap kata berada dalam kalimat.\n",
        "\n",
        "\n",
        "## 2. Model Utama: `Seq2SeqTransformer`\n",
        "\n",
        "Ini adalah kelas utama yang menggabungkan semua bagian menjadi satu model utuh.\n",
        "\n",
        "* **`__init__` (Inisialisasi)**: Di sinilah Anda \"merakit pabriknya\".\n",
        "    1.  Anda memanggil `nn.Transformer` bawaan PyTorch, yang merupakan inti modelnya (berisi tumpukan Encoder dan Decoder). Anda memberinya parameter penting seperti `d_model` (ukuran embedding), `nhead` (jumlah *attention heads*), dan jumlah *layers*.\n",
        "    2.  Anda membuat dua `TokenEmbedding`: satu untuk *source* (Inggris) dan satu untuk *target* (Prancis).\n",
        "    3.  Anda membuat satu `PositionalEncoding`.\n",
        "    4.  Anda membuat `nn.Linear` layer terakhir yang disebut `generator`. Tugasnya adalah mengambil output akhir dari Transformer (sebuah vektor) dan mengubahnya menjadi skor probabilitas untuk *setiap kata* dalam kosakata target (Prancis).\n",
        "\n",
        "* **`forward` (Alur Pelatihan)**: Fungsi ini mendefinisikan apa yang terjadi ketika Anda memberi model data **selama pelatihan**.\n",
        "    1.  Ambil kalimat *source* (src) dan *target* (trg).\n",
        "    2.  Ubah keduanya menjadi *embeddings* (vektor makna).\n",
        "    3.  Tambahkan *positional encoding* ke kedua embeddings tersebut.\n",
        "    4.  Masukkan embeddings yang sudah lengkap ini, beserta *masking* (lihat poin 3), ke dalam `nn.Transformer`.\n",
        "    5.  Ambil output Transformer dan masukkan ke `generator` untuk mendapatkan prediksi akhir.\n",
        "\n",
        "* **`encode` & `decode`**: Ini adalah fungsi terpisah yang biasanya digunakan saat **inferensi** (menerjemahkan kalimat baru), di mana Anda hanya perlu menjalankan Encoder satu kali (`encode`) dan Decoder berulang kali (`decode`) kata per kata.\n",
        "\n",
        "\n",
        "## 3. Aturan Main: Fungsi Masking (`create_mask`)\n",
        "\n",
        "Ini adalah bagian yang sangat penting dan seringkali rumit. *Masking* adalah \"aturan main\" yang memberi tahu model bagian mana dari data yang boleh (atau tidak boleh) dilihat.\n",
        "\n",
        "* **`src_padding_mask` & `tgt_padding_mask` (Masking Padding)**: Ingat di Bagian 1 kita menambahkan token `<pad>` untuk membuat semua kalimat sama panjang? Masking ini memberi tahu model, \"Hei, token `<pad>` ini tidak penting. **Abaikan mereka** saat melakukan perhitungan *self-attention*.\" Ini mencegah model \"belajar\" dari padding buatan.\n",
        "\n",
        "* **`tgt_mask` (Subsequent Mask / Look-Ahead Mask)**: Ini adalah topeng terpenting untuk *Decoder*. Saat melatih model untuk menerjemahkan \"je suis étudiant\", dan model sedang mencoba memprediksi kata \"suis\" (kata ke-2), ia **TIDAK BOLEH** \"curang\" dan melihat jawaban yang benar (\"étudiant\", kata ke-3). Masking ini berbentuk segitiga yang **menyembunyikan semua kata di masa depan** dari posisi saat ini. Ini memaksa model untuk belajar memprediksi kata *selanjutnya* hanya berdasarkan kata *sebelumnya*."
      ],
      "metadata": {
        "id": "YgOt6wNCozUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# BAGIAN 2: FOKUS PENILAIAN 2 - ARSITEKTUR TRANSFORMER (BOBOT 25%)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Memulai Fokus Penilaian 2: Definisi Arsitektur ---\")\n",
        "\n",
        "# --- 2.1. Class PositionalEncoding ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# --- 2.2. Class TokenEmbedding ---\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# --- 2.3. Class Seq2SeqTransformer (Model Utama) ---\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead,\n",
        "                 src_vocab_size, tgt_vocab_size, dim_feedforward=512, dropout=0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=emb_size,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=False\n",
        "        )\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, trg, src_mask, tgt_mask,\n",
        "                src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(\n",
        "            src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "            src_padding_mask, tgt_padding_mask, memory_key_padding_mask\n",
        "        )\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.transformer.encoder(\n",
        "            self.positional_encoding(self.src_tok_emb(src)), src_mask\n",
        "        )\n",
        "\n",
        "    def decode(self, tgt, memory, tgt_mask):\n",
        "        return self.transformer.decoder(\n",
        "            self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask\n",
        "        )\n",
        "\n",
        "# --- 2.4. Fungsi Pembuatan Masking ---\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
        "\n",
        "print(\"Class dan fungsi arsitektur berhasil didefinisikan.\")\n",
        "print(\"--- Selesai Fokus Penilaian 2 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb9y0vX1D6Ib",
        "outputId": "42b75218-ff95-4154-ef61-72197e07768c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Memulai Fokus Penilaian 2: Definisi Arsitektur ---\n",
            "Class dan fungsi arsitektur berhasil didefinisikan.\n",
            "--- Selesai Fokus Penilaian 2 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ini adalah **BAGIAN 3**, \"ruang mesin\"  dari proyek Anda, di mana proses **pelatihan model** yang sebenarnya terjadi. Setelah data (Bagian 1) dan arsitektur (Bagian 2) siap, di sinilah model Anda \"belajar\" dari data.\n",
        "\n",
        "Berikut adalah rincian prosesnya:\n",
        "\n",
        "## 1. Inisialisasi (Persiapan Pelatihan)\n",
        "\n",
        "Sebelum pelatihan dimulai, Anda perlu menyiapkan tiga komponen utama:\n",
        "\n",
        "1.  **Model**: Anda membuat instansi dari kelas `Seq2SeqTransformer` yang Anda definisikan di Bagian 2.\n",
        "    * **Penting**: Anda sengaja menggunakan *hyperparameter* yang **lebih kecil** (misalnya `EMB_SIZE = 256`, `FFN_HID_DIM = 256`). Ini adalah langkah praktis yang cerdas untuk **mengatasi error OOM (Out of Memory)**, terutama di lingkungan dengan VRAM terbatas seperti Colab. Model yang lebih ringan membutuhkan lebih sedikit memori.\n",
        "    * Model kemudian \"diterapkan\" (`.apply(init_weights)`) dengan inisialisasi bobot (weights) yang baik (menggunakan *Xavier* dan *Normal distribution*) untuk membantu pelatihan berjalan lebih stabil.\n",
        "    * Terakhir, model dipindahkan ke `DEVICE` (`.to(DEVICE)`) agar berjalan di GPU (`'cuda'`) jika tersedia, yang jauh lebih cepat.\n",
        "\n",
        "2.  **Loss Function (Fungsi Kerugian)**: Anda mendefinisikan \"juri\" atau \"penilai\" untuk model.\n",
        "    * `nn.CrossEntropyLoss`: Ini adalah pilihan standar untuk tugas klasifikasi (dan memprediksi kata berikutnya pada dasarnya adalah klasifikasi di antara semua kata di *vocabulary*).\n",
        "    * `ignore_index=PAD_IDX`: Ini adalah pengaturan yang **sangat penting**. Ini memberi tahu \"juri\" untuk **mengabaikan** token `<pad>` saat menghitung *error*. Ini logis, karena kita tidak peduli jika model salah memprediksi *padding*; kita hanya peduli pada kata-kata yang sebenarnya.\n",
        "\n",
        "3.  **Optimizer (Pengoptimal)**: Anda memilih \"mekanik\" yang akan memperbarui bobot model.\n",
        "    * `optim.Adam`: Ini adalah algoritma optimasi yang paling umum digunakan, dikenal cepat dan efisien. Ia mengambil *error* (loss) yang dihitung oleh \"juri\" dan menggunakannya untuk menyesuaikan *setiap* parameter di dalam model secara cerdas agar kinerjanya menjadi sedikit lebih baik.\n",
        "\n",
        "## 2. Fungsi Evaluasi (Kartu Rapor Model)\n",
        "\n",
        "Anda mendefinisikan fungsi `evaluate` terpisah yang bertujuan untuk **mengukur performa model pada data yang belum pernah dilihatnya** (data validasi).\n",
        "\n",
        "* `model.eval()`: Mengubah model ke mode \"evaluasi\", yang menonaktifkan fitur-fitur yang hanya dipakai saat latihan (seperti *dropout*).\n",
        "* `with torch.no_grad()`: Perintah ini **mematikan perhitungan gradien**. Ini sangat penting karena saat evaluasi, Anda tidak perlu menghitung \"siapa yang salah\" (gradien), Anda hanya ingin tahu hasilnya. Ini menghemat banyak memori dan mempercepat proses.\n",
        "* **Perhitungan Akurasi**: Selain menghitung *loss* (seberapa salah), fungsi ini juga menghitung *accuracy* (seberapa sering tebakannya benar). Ia melakukannya dengan mengambil prediksi (`.argmax()`), membandingkannya dengan jawaban sebenarnya, dan **hanya menghitung token yang bukan `<pad>`**.\n",
        "\n",
        "## 3. Loop Pelatihan Utama (Proses Belajar)\n",
        "\n",
        "Ini adalah inti dari proses belajar, yang diatur untuk berjalan hanya **1 epoch** (satu kali melihat keseluruhan data latih).\n",
        "\n",
        "Loop ini terdiri dari dua fase:\n",
        "\n",
        "### Fase 1: Training (Latihan)\n",
        "Model diatur ke `model.train()` (mengaktifkan kembali *dropout*, dll.). Kemudian, ia mengulangi setiap *batch* data dari `train_dataloader`. Untuk setiap *batch*:\n",
        "\n",
        "1.  **Persiapan Data**: Input target dipecah menjadi `tgt_input` (apa yang \"dilihat\" *decoder*, misal: `[<bos>, \"je\", \"suis\"]`) dan `tgt_out` (apa yang harus \"ditebak\" *decoder*, misal: `[\"je\", \"suis\", <eos>]`). Ini disebut *teacher forcing*.\n",
        "2.  **Forward Pass**: Data (`src` dan `tgt_input`) dan *masking* dimasukkan ke model (`logits = model(...)`). Model membuat prediksi (logits) untuk setiap kata dalam *batch*.\n",
        "3.  **Hitung Loss**: `loss = loss_fn(...)` dihitung dengan membandingkan prediksi model (`logits`) dengan jawaban yang benar (`tgt_out`).\n",
        "4.  **Backward Pass (Belajar)**:\n",
        "    * `optimizer.zero_grad()`: Menghapus sisa perhitungan dari *batch* sebelumnya.\n",
        "    * `loss.backward()`: Menghitung gradien, atau \"menyebarkan kesalahan\" ke seluruh jaringan untuk mencari tahu parameter mana yang paling berkontribusi pada kesalahan.\n",
        "    * `optimizer.step()`: \"Mekanik\" (Adam) menggunakan gradien tadi untuk **memperbarui bobot (weights)** model. Di sinilah proses \"belajar\" yang sesungguhnya terjadi.\n",
        "\n",
        "Setiap 50 *batch*, *loss* sementara dicetak agar Anda bisa memantau kemajuan.\n",
        "\n",
        "### Fase 2: Validation (Validasi)\n",
        "**Setelah** semua *batch* latihan dalam satu epoch selesai, kode ini:\n",
        "\n",
        "1.  **Memanggil `evaluate(...)`**: Menjalankan fungsi \"kartu rapor\" yang Anda buat tadi pada **seluruh data validasi** (`val_dataloader`).\n",
        "2.  **Mencetak Laporan**: Ini mencetak ringkasan kinerja akhir untuk epoch tersebut, yang berisi:\n",
        "    * `TrainLoss`: Rata-rata *error* pada data yang dilihatnya saat latihan.\n",
        "    * `ValLoss`: Rata-rata *error* pada data \"ujian\" (validasi).\n",
        "    * `ValAcc`: Akurasi (persentase tebakan benar) pada data \"ujian\".\n",
        "\n",
        "Ini adalah metrik terpenting untuk mengetahui apakah model Anda benar-benar belajar atau hanya menghafal."
      ],
      "metadata": {
        "id": "4ptko8EKpNGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# BAGIAN 3: FOKUS PENILAIAN 3 - PROSES PELATIHAN (BOBOT 35%)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Memulai Fokus Penilaian 3: Proses Pelatihan ---\")\n",
        "\n",
        "# --- 3.1. Inisialisasi Model, Loss, dan Optimizer ---\n",
        "\n",
        "# --- PERUBAHAN HYPERPARAMETER UNTUK MENGATASI OOM ---\n",
        "# Kita gunakan arsitektur yang lebih kecil (ringan)\n",
        "EMB_SIZE = 256       # (dari 512)\n",
        "NHEAD = 8            # (256 % 8 == 0, jadi ini OK)\n",
        "FFN_HID_DIM = 256    # (dari 512)\n",
        "NUM_ENCODER_LAYERS = 3 # (dari 3, ini OK)\n",
        "NUM_DECODER_LAYERS = 3 # (dari 3, ini OK)\n",
        "LEARNING_RATE = 0.0001\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# Hyperparameter Training (Sesuai Permintaan)\n",
        "NUM_EPOCHS = 1 # Sesuai permintaan, hanya 1 epoch\n",
        "\n",
        "# Inisialisasi model\n",
        "model = Seq2SeqTransformer(\n",
        "    NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD,\n",
        "    SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM, DROPOUT\n",
        ").to(DEVICE)\n",
        "\n",
        "# Inisialisasi bobot\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.Embedding):\n",
        "        nn.init.normal_(m.weight, mean=0, std=0.02)\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Loss function\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "print(\"Model (arsitektur lebih ringan), Loss, dan Optimizer berhasil diinisialisasi.\")\n",
        "\n",
        "# --- 3.2. Fungsi Evaluasi (Untuk ValLoss & ValAcc) ---\n",
        "def evaluate(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in dataloader:\n",
        "            src = src.to(DEVICE)\n",
        "            tgt = tgt.to(DEVICE)\n",
        "\n",
        "            tgt_input = tgt[:-1, :]\n",
        "            tgt_out = tgt[1:, :]\n",
        "\n",
        "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "            logits = model(\n",
        "                src, tgt_input,\n",
        "                src_mask, tgt_mask,\n",
        "                src_padding_mask, tgt_padding_mask,\n",
        "                src_padding_mask\n",
        "            )\n",
        "\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # --- Hitung Akurasi ---\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            non_pad_mask = (tgt_out != PAD_IDX)\n",
        "            correct = (pred == tgt_out) & non_pad_mask\n",
        "\n",
        "            total_correct += correct.sum().item()\n",
        "            total_tokens += non_pad_mask.sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "    return avg_loss, avg_accuracy\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# BAGIAN 3.3: Loop Training Utama (MODIFIKASI - Validasi di Akhir Epoch)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Memulai Loop Training (1 Epoch) ---\")\n",
        "print(f\"Total batch per epoch: {len(train_dataloader)}\")\n",
        "# Hapus peringatan validasi per batch\n",
        "\n",
        "start_time_epoch = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    model.train() # Set model ke mode training\n",
        "    epoch_loss = 0 # Kita akan kumpulkan total loss di sini\n",
        "\n",
        "    print(f\"\\n--- Memulai Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    # 1. LOOP TRAINING BATCH\n",
        "    for i, (src, tgt) in enumerate(train_dataloader):\n",
        "        start_time_batch = time.time()\n",
        "\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "        tgt_out = tgt[1:, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(\n",
        "            src, tgt_input,\n",
        "            src_mask, tgt_mask,\n",
        "            src_padding_mask, tgt_padding_mask,\n",
        "            src_padding_mask\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss_batch = loss.item()\n",
        "        epoch_loss += train_loss_batch\n",
        "\n",
        "        # --- Modifikasi ---\n",
        "        # Kita tidak lagi menjalankan validasi di sini.\n",
        "        # Kita hanya akan print loss training sesekali agar terlihat progress.\n",
        "        if (i + 1) % 50 == 0: # Print progress setiap 50 batch\n",
        "             print(f\"  Epoch {epoch+1} | Batch {i+1}/{len(train_dataloader)} | TrainLoss (batch): {train_loss_batch:.4f}\")\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # --- DOKUMENTASI PROSES PELATIHAN (SESUAI PERMINTAAN) ---\n",
        "    # -----------------------------------------------------------------\n",
        "    # 2. LOOP VALIDASI (Jalankan SETELAH epoch selesai)\n",
        "\n",
        "    # Hitung rata-rata TrainLoss untuk epoch ini\n",
        "    avg_train_loss = epoch_loss / len(train_dataloader)\n",
        "\n",
        "    print(\"\\n--- Selesai Epoch, Menjalankan Validasi ---\")\n",
        "    start_time_val = time.time()\n",
        "\n",
        "    # Panggil fungsi evaluate satu kali\n",
        "    val_loss, val_acc = evaluate(model, val_dataloader, loss_fn)\n",
        "\n",
        "    val_time = time.time() - start_time_val\n",
        "\n",
        "    # Tampilkan semua metrik di akhir epoch\n",
        "    print(f\"--- Laporan Akhir Epoch {epoch+1} ---\")\n",
        "    print(f\"Waktu Validasi: {val_time:.2f} detik\")\n",
        "    print(f\"  TrainLoss (Rata-rata): {avg_train_loss:.4f}\")\n",
        "    print(f\"  ValLoss (Epoch): {val_loss:.4f}\")\n",
        "    print(f\"  ValAcc  (Epoch): {val_acc*100:.2f}%\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- Laporan Akhir Training ---\n",
        "end_time_epoch = time.time()\n",
        "epoch_mins = (end_time_epoch - start_time_epoch) / 60\n",
        "\n",
        "print(\"\\n--- Selesai Fokus Penilaian 3 ---\")\n",
        "print(f\"Total Waktu Training ({NUM_EPOCHS} Epoch): {epoch_mins:.2f} menit\")\n",
        "# Laporan final sudah dicetak di dalam loop di atas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgZxpVMsEDX2",
        "outputId": "20617dbe-0f6a-4f12-8e50-9f498472b0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Memulai Fokus Penilaian 3: Proses Pelatihan ---\n",
            "Model (arsitektur lebih ringan), Loss, dan Optimizer berhasil diinisialisasi.\n",
            "\n",
            "--- Memulai Loop Training (1 Epoch) ---\n",
            "Total batch per epoch: 1939\n",
            "\n",
            "--- Memulai Epoch 1/1 ---\n",
            "  Epoch 1 | Batch 50/1939 | TrainLoss (batch): 3.2297\n",
            "  Epoch 1 | Batch 100/1939 | TrainLoss (batch): 2.1783\n",
            "  Epoch 1 | Batch 150/1939 | TrainLoss (batch): 1.5632\n",
            "  Epoch 1 | Batch 200/1939 | TrainLoss (batch): 1.3448\n",
            "  Epoch 1 | Batch 250/1939 | TrainLoss (batch): 1.1033\n",
            "  Epoch 1 | Batch 300/1939 | TrainLoss (batch): 0.9203\n",
            "  Epoch 1 | Batch 350/1939 | TrainLoss (batch): 0.7739\n",
            "  Epoch 1 | Batch 400/1939 | TrainLoss (batch): 0.7295\n",
            "  Epoch 1 | Batch 450/1939 | TrainLoss (batch): 0.5497\n",
            "  Epoch 1 | Batch 500/1939 | TrainLoss (batch): 0.4479\n",
            "  Epoch 1 | Batch 550/1939 | TrainLoss (batch): 0.3861\n",
            "  Epoch 1 | Batch 600/1939 | TrainLoss (batch): 0.3129\n",
            "  Epoch 1 | Batch 650/1939 | TrainLoss (batch): 0.2763\n",
            "  Epoch 1 | Batch 700/1939 | TrainLoss (batch): 0.3492\n",
            "  Epoch 1 | Batch 750/1939 | TrainLoss (batch): 0.1733\n",
            "  Epoch 1 | Batch 800/1939 | TrainLoss (batch): 0.1910\n",
            "  Epoch 1 | Batch 850/1939 | TrainLoss (batch): 0.1302\n",
            "  Epoch 1 | Batch 900/1939 | TrainLoss (batch): 0.1329\n",
            "  Epoch 1 | Batch 950/1939 | TrainLoss (batch): 0.2106\n",
            "  Epoch 1 | Batch 1000/1939 | TrainLoss (batch): 0.1264\n",
            "  Epoch 1 | Batch 1050/1939 | TrainLoss (batch): 0.1654\n",
            "  Epoch 1 | Batch 1100/1939 | TrainLoss (batch): 0.1356\n",
            "  Epoch 1 | Batch 1150/1939 | TrainLoss (batch): 0.0969\n",
            "  Epoch 1 | Batch 1200/1939 | TrainLoss (batch): 0.1503\n",
            "  Epoch 1 | Batch 1250/1939 | TrainLoss (batch): 0.0717\n",
            "  Epoch 1 | Batch 1300/1939 | TrainLoss (batch): 0.1472\n",
            "  Epoch 1 | Batch 1350/1939 | TrainLoss (batch): 0.1160\n",
            "  Epoch 1 | Batch 1400/1939 | TrainLoss (batch): 0.0901\n",
            "  Epoch 1 | Batch 1450/1939 | TrainLoss (batch): 0.0925\n",
            "  Epoch 1 | Batch 1500/1939 | TrainLoss (batch): 0.1148\n",
            "  Epoch 1 | Batch 1550/1939 | TrainLoss (batch): 0.1062\n",
            "  Epoch 1 | Batch 1600/1939 | TrainLoss (batch): 0.0964\n",
            "  Epoch 1 | Batch 1650/1939 | TrainLoss (batch): 0.0639\n",
            "  Epoch 1 | Batch 1700/1939 | TrainLoss (batch): 0.0830\n",
            "  Epoch 1 | Batch 1750/1939 | TrainLoss (batch): 0.0990\n",
            "  Epoch 1 | Batch 1800/1939 | TrainLoss (batch): 0.0929\n",
            "  Epoch 1 | Batch 1850/1939 | TrainLoss (batch): 0.0715\n",
            "  Epoch 1 | Batch 1900/1939 | TrainLoss (batch): 0.0481\n",
            "\n",
            "--- Selesai Epoch, Menjalankan Validasi ---\n",
            "--- Laporan Akhir Epoch 1 ---\n",
            "Waktu Validasi: 4.87 detik\n",
            "  TrainLoss (Rata-rata): 0.4930\n",
            "  ValLoss (Epoch): 0.0537\n",
            "  ValAcc  (Epoch): 98.25%\n",
            "----------------------------------------\n",
            "\n",
            "--- Selesai Fokus Penilaian 3 ---\n",
            "Total Waktu Training (1 Epoch): 1.28 menit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ini adalah **BAGIAN 4**, tahap akhir yang disebut **Inferensi (Penerapan)**.\n",
        "\n",
        "Setelah menyiapkan data (Bagian 1), merancang arsitektur (Bagian 2), dan melatih model (Bagian 3), kini saatnya **menggunakan model yang sudah dilatih** tersebut untuk melakukan tugas sebenarnya: menerjemahkan kalimat baru.\n",
        "\n",
        "Berikut adalah rincian dari proses ini:\n",
        "\n",
        "## 1. Fungsi `greedy_decode` (Inti Penerjemah)\n",
        "\n",
        "Ini adalah fungsi \"inti\" yang menghasilkan terjemahan kata per kata. Prosesnya berbeda dengan saat pelatihan:\n",
        "\n",
        "1.  **Encoder Dulu**: Model mengambil kalimat *source* (Inggris) dan menjalankannya melalui **Encoder** *hanya satu kali*. Hasilnya adalah `memory`, sebuah representasi makna dari kalimat sumber.\n",
        "2.  **Decoder Mulai**: *Decoder* mulai dengan satu token, yaitu `<bos>` (awal kalimat).\n",
        "3.  **Loop Generasi**: Model masuk ke dalam *loop* untuk menghasilkan kalimat *target* (Prancis):\n",
        "    * Ia mengambil `memory` (dari Inggris) dan kalimat Prancis yang sudah dihasilkan *sejauh ini* (awalnya hanya `<bos>`).\n",
        "    * Ia bertanya pada **Decoder**: \"Berdasarkan ini, apa kata *selanjutnya*?\"\n",
        "    * Model memberikan probabilitas untuk setiap kata di *vocabulary* Prancis.\n",
        "    * **\"Greedy\"**: Fungsi ini serakah. Ia tidak berpikir panjang; ia langsung memilih **satu kata dengan probabilitas tertinggi** (menggunakan `torch.max`).\n",
        "    * Ia menambahkan kata baru ini ke kalimat yang sedang dibuat.\n",
        "4.  **Berhenti**: Loop ini berlanjut (kata baru ditambahkan, lalu dipakai untuk memprediksi kata *berikutnya*, dan seterusnya) sampai model memprediksi token `<eos>` (akhir kalimat) atau mencapai batas panjang maksimum.\n",
        "\n",
        "## 2. Fungsi `translate` (Pembungkus yang Ramah)\n",
        "\n",
        "Fungsi `greedy_decode` bekerja dengan angka (tensor). Fungsi `translate` adalah \"pembungkus\" (wrapper) yang membuatnya **mudah digunakan oleh manusia**.\n",
        "\n",
        "Fungsi ini menangani seluruh proses:\n",
        "\n",
        "1.  **Pre-processing (Input)**: Mengambil kalimat mentah Bahasa Inggris (string), mengubahnya menjadi token, lalu mengubah token menjadi angka (indeks *vocabulary*), dan menambah token `<bos>`/`<eos>`.\n",
        "2.  **Menjalankan Model**: Memanggil fungsi `greedy_decode` (inti penerjemah di atas) untuk mendapatkan hasil terjemahan dalam bentuk angka/tensor.\n",
        "3.  **Post-processing (Output)**: Mengambil tensor angka dari model, mengubahnya kembali menjadi kata-kata (menggunakan *vocabulary* target/Prancis), dan membersihkan token `<bos>`/`<eos>` agar menjadi kalimat yang rapi dan bisa dibaca manusia.\n",
        "\n",
        "## 3. Eksekusi Contoh (Test Drive)\n",
        "\n",
        "Ini adalah bagian \"test drive\" di mana Anda akhirnya mencoba model Anda:\n",
        "\n",
        "* **Mode `.eval()`**: Model diatur ke mode evaluasi, yang menonaktifkan *dropout* dan fitur pelatihan lainnya untuk hasil yang konsisten.\n",
        "* **Contoh Validasi**: Kode ini mengambil dua contoh dari *validation set* (data yang tidak pernah dilihat model saat latihan). Ini adalah tes yang adil. Ia mencetak kalimat Inggris (Source), terjemahan yang benar (Target), dan terjemahan hasil tebakan model (Model).\n",
        "* **Contoh Kustom**: Kode ini juga mencoba menerjemahkan kalimat yang benar-benar baru (\"he is never cold in december .\") untuk melihat bagaimana model menangani input yang tidak terduga.\n",
        "\n",
        "**Peringatan Penting**: Seperti yang dicatat dalam kode, karena model **hanya dilatih 1 epoch** (untuk menghemat waktu dan memori), hasil terjemahannya kemungkinan besar **masih akan buruk** atau tidak masuk akal. Namun, yang terpenting adalah **proses inferensinya berjalan dengan benar** dari awal hingga akhir."
      ],
      "metadata": {
        "id": "u2cOIRlnp18-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# BAGIAN 4: FOKUS PENILAIAN 4 - PENERAPAN (INFERENSI) (BOBOT 20%)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Memulai Fokus Penilaian 4: Inferensi ---\")\n",
        "\n",
        "# --- 4.1. Fungsi Greedy Decode ---\n",
        "# (Tidak ada perubahan, fungsi ini generik)\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "    memory = model.encode(src, src_mask)\n",
        "    memory = memory.to(DEVICE)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "\n",
        "    for i in range(max_len - 1):\n",
        "        tgt_seq_len = ys.size(0)\n",
        "        tgt_mask = (generate_square_subsequent_mask(tgt_seq_len)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "# --- 4.2. Fungsi Wrapper 'translate' ---\n",
        "# (Tidak ada perubahan, fungsi ini generik)\n",
        "def translate(model, src_sentence, vocab_src, vocab_tgt, tokenize_src):\n",
        "    model.eval()\n",
        "    tokens = [BOS_IDX] + [vocab_src[token] for token in tokenize_src(src_sentence)] + [EOS_IDX]\n",
        "    src = torch.LongTensor(tokens).unsqueeze(1).to(DEVICE)\n",
        "    src_mask = (torch.zeros(src.shape[0], src.shape[0])).type(torch.bool).to(DEVICE)\n",
        "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=50, start_symbol=BOS_IDX)\n",
        "    tgt_words = [vocab_tgt.get_itos()[tok] for tok in tgt_tokens.flatten()]\n",
        "    return \" \".join(tgt_words).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").strip()\n",
        "\n",
        "# --- 4.3. Eksekusi Contoh Inferensi ---\n",
        "# PENTING: Karena training hanya 1 epoch, hasil terjemahan\n",
        "# kemungkinan besar masih akan BURUK.\n",
        "# Namun, dengan dataset kecil ini, hasilnya mungkin sedikit lebih baik\n",
        "# daripada model OOM Anda sebelumnya.\n",
        "\n",
        "print(\"\\n--- Menjalankan Contoh Terjemahan ---\")\n",
        "\n",
        "# Ambil beberapa contoh dari data validasi untuk perbandingan\n",
        "print(\"\\nContoh 1 (dari data validasi):\")\n",
        "src_text_1 = val_df.iloc[0]['en']\n",
        "tgt_text_1 = val_df.iloc[0]['fr'] # <-- BERUBAH: 'fr'\n",
        "translation_1 = translate(model, src_text_1, vocab_src, vocab_tgt, tokenize_en)\n",
        "print(f\"  EN (Source): {src_text_1}\")\n",
        "print(f\"  FR (Target): {tgt_text_1}\")  # <-- BERUBAH: FR\n",
        "print(f\"  FR (Model) : {translation_1}\")  # <-- BERUBAH: FR\n",
        "\n",
        "\n",
        "print(\"\\nContoh 2 (dari data validasi):\")\n",
        "src_text_2 = val_df.iloc[1]['en']\n",
        "tgt_text_2 = val_df.iloc[1]['fr'] # <-- BERUBAH: 'fr'\n",
        "translation_2 = translate(model, src_text_2, vocab_src, vocab_tgt, tokenize_en)\n",
        "print(f\"  EN (Source): {src_text_2}\")\n",
        "print(f\"  FR (Target): {tgt_text_2}\")  # <-- BERUBAH: FR\n",
        "print(f\"  FR (Model) : {translation_2}\")  # <-- BERUBAH: FR\n",
        "\n",
        "print(\"\\nContoh 3 (Input Kustom):\")\n",
        "src_text_3 = \"he is never cold in december .\"\n",
        "translation_3 = translate(model, src_text_3, vocab_src, vocab_tgt, tokenize_en)\n",
        "print(f\"  EN (Source): {src_text_3}\")\n",
        "print(f\"  FR (Model) : {translation_3}\")  # <-- BERUBAH: FR\n",
        "\n",
        "print(\"\\n--- Selesai Fokus Penilaian 4 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g16xczwdEHDJ",
        "outputId": "9ecc4c18-5549-44d0-dce4-2f871f023b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Memulai Fokus Penilaian 4: Inferensi ---\n",
            "\n",
            "--- Menjalankan Contoh Terjemahan ---\n",
            "\n",
            "Contoh 1 (dari data validasi):\n",
            "  EN (Source): the mango is their most loved fruit , but the apple is my most loved .\n",
            "  FR (Target): la mangue est leur fruit le plus cher , mais la pomme est mon plus aimé .\n",
            "  FR (Model) : la mangue est leur fruit le plus cher , mais la pomme est mon plus aimé .\n",
            "\n",
            "Contoh 2 (dari data validasi):\n",
            "  EN (Source): china is sometimes busy during july , and it is sometimes rainy in march .\n",
            "  FR (Target): la chine est parfois occupé en juillet , et il est parfois pluvieux en mars .\n",
            "  FR (Model) : chine est parfois occupé en juillet , et il est parfois pluvieux en mars .\n",
            "\n",
            "Contoh 3 (Input Kustom):\n",
            "  EN (Source): he is never cold in december .\n",
            "  FR (Model) : il ne fait jamais froid en décembre .\n",
            "\n",
            "--- Selesai Fokus Penilaian 4 ---\n"
          ]
        }
      ]
    }
  ]
}
